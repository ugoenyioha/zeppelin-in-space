#!/usr/bin/env bash
# bin/compile <build-dir> <cache-dir> <env-dir>
# heroku inline buildpack

set -e

build_dir=$1
cache_dir=$2
env_dir=$3
bp_dir=$(dirname $(dirname $0))


notice() {
  echo "-----> $@"
}

start() {
  echo -n "-----> $@..."
}

finish() {
  echo " Done."
}

cd $build_dir

start Installing Zeppelin
echo ""
curl -s http://apache.spinellicreations.com/zeppelin/zeppelin-0.8.1/zeppelin-0.8.1-bin-netinst.tgz | tar xz
mv zeppelin-0.8.1-bin-netinst zeppelin-home
notice SLF4J warnings are ok
#$build_dir/zeppelin-home/bin/install-interpreter.sh --name postgresql 2>&1 | sed  's/^/         /'
#rm -rf  $build_dir/zeppelin-home/interpreter/spark/dep
rm -rf  $build_dir/zeppelin-home/interpreter/spark/R
rm -rf  $build_dir/zeppelin-home/interpreter/spark/pyspark
rm -rf  $build_dir/zeppelin-home/tmp
mv $bp_dir/conf/zeppelin-env.sh $build_dir/zeppelin-home/conf
mv $bp_dir/conf/zeppelin-site.xml $build_dir/zeppelin-home/conf

notice cleanup
rm -rf zeppelin-0.8.1-bin-netinst.tgz

start Installing s3n:// and s3a:// HDFS Support
mkdir -p $build_dir/spark-home/lib
curl -s -L https://s3.amazonaws.com/heroku-spark/libhadoop.so.1.0.0 > $build_dir/spark-home/lib/libhadoop.so
curl -s -L https://s3.amazonaws.com/heroku-spark/libhdfs.so.0.0.0 > $build_dir/spark-home/lib/libhdfs.so
curl -s -L https://s3.amazonaws.com/heroku-spark/$hadoop_aws_shade > $build_dir/spark-home/lib/hadoop-aws-shaded.jar
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/liblzo2.so.2.0.0 > $build_dir/spark-home/lib/liblzo2.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/libgplcompression.so.0.0.0 > $build_dir/spark-home/lib/libgplcompression.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/hadoop-lzo-0.4.20-SNAPSHOT.jar > $build_dir/spark-home/lib/hadoop-lzo.jar
finish Installing s3a:// HDFS Support

notice Done.

